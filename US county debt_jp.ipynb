{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b19a10c5-4765-4f95-b4fa-b4d4faf11142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table found. Proceeding to extract data.\n",
      "Rows found in the table. Extracting county names.\n",
      "Number of county names extracted: 3137\n",
      "Number of rows in the dataset: 313819\n",
      "Number of unique counties: 3137\n",
      "Number of rows per county: 100\n",
      "Data saved to C:\\JN\\dti_data_with_county_names.csv\n",
      "Data extraction, merging, and formatting complete.\n",
      "   year  area_fips        dti  County Name\n",
      "0  1999       1001  1.82-2.15  Autauga, AL\n",
      "1  1999       1003  1.82-2.15  Baldwin, AL\n",
      "2  1999       1005   0.0-0.78  Barbour, AL\n",
      "3  1999       1007  2.61-3.43     Bibb, AL\n",
      "4  1999       1009  1.82-2.15   Blount, AL\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Correct path to the Chrome browser executable\n",
    "chrome_path = \"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\"\n",
    "\n",
    "# Set up Selenium WebDriver for Chrome\n",
    "options = Options()\n",
    "options.binary_location = chrome_path\n",
    "\n",
    "# Correct path to the ChromeDriver executable\n",
    "driver_path = 'C:\\\\JN\\\\chromedriver.exe'  \n",
    "# Initialize the Service object with the driver path\n",
    "service = Service(executable_path=driver_path)\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# URL of the webpage containing the county names\n",
    "url = 'https://www.federalreserve.gov/releases/z1/dataviz/household_debt/county/table/'\n",
    "\n",
    "# Fetching the webpage content using Selenium\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "time.sleep(5)  \n",
    "# Get the page source and parse it with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "driver.quit()\n",
    "\n",
    "# Find the table containing the county names\n",
    "table = soup.find('table')\n",
    "\n",
    "# Check if the table is found\n",
    "if table is not None:\n",
    "    print(\"Table found. Proceeding to extract data.\")\n",
    "    \n",
    "    # Extract county names from the third column of the table\n",
    "    county_names = []\n",
    "    rows = table.find_all('tr')\n",
    "    \n",
    "    # Checking if rows are found\n",
    "    if len(rows) > 1:\n",
    "        print(\"Rows found in the table. Extracting county names.\")\n",
    "        \n",
    "        for row in rows[1:]:  # Skip the header row\n",
    "            cols = row.find_all('td')\n",
    "            \n",
    "            # Ensuring there are enough columns in the row\n",
    "            if len(cols) > 2:\n",
    "                county_name = cols[2].text.strip()\n",
    "                county_names.append(county_name)\n",
    "            else:\n",
    "                print(\"Row does not have enough columns. Skipping row.\")\n",
    "        \n",
    "        # Converting to DataFrame\n",
    "        county_df = pd.DataFrame(county_names, columns=['County Name'])\n",
    "        \n",
    "        # Display the number of county names extracted\n",
    "        print(f\"Number of county names extracted: {len(county_names)}\")\n",
    "    else:\n",
    "        print(\"No rows found in the table.\")\n",
    "else:\n",
    "    print(\"Table not found on the webpage.\")\n",
    "\n",
    "# Load the existing CSV file\n",
    "file_path = 'C:\\\\Users\\\\THINKPAD\\\\Downloads\\\\household-debt-by-county\\\\household-debt-by-county.csv'  # Backend Data\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Droping the 'quarter' column\n",
    "df = df.drop(columns=['qtr'])\n",
    "\n",
    "# Combine the 'Lower bound' and 'Upper bound' columns into a new column 'dti'\n",
    "df['dti'] = df['low'].astype(str) + '-' + df['high'].astype(str)\n",
    "\n",
    "# Drop the original 'Lower bound' and 'Upper bound' columns\n",
    "df = df.drop(columns=['low', 'high'])\n",
    "\n",
    "# Display the number of rows in the dataset\n",
    "print(f\"Number of rows in the dataset: {len(df)}\")\n",
    "\n",
    "# Creating a mapping of unique counties to their names\n",
    "county_mapping = dict(enumerate(county_df['County Name']))\n",
    "\n",
    "# Calculating how many times each county should be repeated\n",
    "repeat_factor = len(df) // len(county_mapping)\n",
    "\n",
    "# Creating a new column 'County Name' in the original dataset\n",
    "df['County Name'] = df.index.map(lambda x: county_mapping[x % len(county_mapping)])\n",
    "\n",
    "# Display some information\n",
    "print(f\"Number of unique counties: {len(county_mapping)}\")\n",
    "print(f\"Number of rows per county: {repeat_factor}\")\n",
    "\n",
    "# Save the merged data to a new CSV file\n",
    "output_path = 'C:\\\\JN\\\\dti_data_with_county_names.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Data saved to {output_path}\")\n",
    "print(\"Data extraction, merging, and formatting complete.\")\n",
    "\n",
    "# to verify the result\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aac843e-d1bf-42ed-a4bf-a73ecd03478a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
